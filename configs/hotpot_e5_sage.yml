experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: mteb/hotpotqa # Use 'mteb/hotpotqa' for MTEB format or 'hotpot_qa' for legacy
  graph_dir: ./datasets/hotpot
  graph_name: page_graph
  chunk_size: 220
  chunk_overlap: 40
  batch_size: 2048 # Batch size for inference/embedding per GPU (2x L40S = 4096 total)
  eval_batch_size: 2048 # Batch size for evaluation queries per GPU (no gradients, scaled by GPU count)
  semantic_k: 5 # kNN augmentation: add k nearest neighbors in embedding space (set to null to disable)
  knn_only: false # If true, use only semantic edges (ablation)
  knn_index_type: hnsw # Index type for kNN search: "flat" (exact, slow) or "hnsw" (approximate, fast)

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0
  batch_size: 128

gnn:
  type: graphsage
  layers: 1
  hidden_dim: 768
  dropout: 0.1
  fanouts: [5]

loss:
  tau: 0.1
  tau_graph: 0.2
  lambda_q2d: 0.7
  alpha_link: 0.0

train:
  query_batch_size: 32
  gradient_accumulation_steps: 4
  epochs: 1
  lr_encoder: 2.0e-5
  lr_gnn: 1.0e-3
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 20
  eval_every_steps: 500

eval:
  num_samples: 500
  num_negatives: 300
  recall_k: 10
  query_batch_size: 64

index:
  type: hnsw # Index type: "flat", "hnsw", or "ivf"
  hnsw_m: 32
  hnsw_ef_construction: 40
  hnsw_ef_search: 16
  ivf_nlist: 4096
  ivf_nprobe: 32
  topk: 50
