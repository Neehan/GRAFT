experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: mteb/hotpotqa # Use 'mteb/hotpotqa' for MTEB format or 'hotpot_qa' for legacy
  graph_dir: ./datasets/hotpot
  graph_name: page_graph
  chunk_size: 220
  chunk_overlap: 40
  batch_size: 2048 # Total batch size for inference/embedding (will be split across available GPUs with DataParallel)
  eval_batch_size: 2048 # Total batch size for evaluation queries (DataParallel splits encoding across GPUs)
  semantic_k: 3 # kNN augmentation: add k nearest neighbors in embedding space (set to null to disable)
  knn_only: true # If true, use only semantic edges (ablation)
  knn_index_type: hnsw # Index type for kNN search: "flat" (exact, slow) or "hnsw" (approximate, fast)

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0

graph:
  fanouts: [5, 2] # Neighborhood sampling for graph-aware training

loss:
  tau: 0.1
  tau_graph: 0.2
  lambda_q2d: 0.9 # Primary retrieval objective (graph structure via neighbor loss)
  alpha_link: 0.0

train:
  query_batch_size: 8
  gradient_accumulation_steps: 8
  epochs: 3
  lr_encoder: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 20
  eval_every_steps: 100

eval:
  num_samples: 500
  num_negatives: 200
  recall_k: 2
  query_batch_size: 16
  encoder_batch_size: 128

index:
  type: hnsw # Index type: "flat", "hnsw", or "ivf"
  quantize: true # Use SQ8 quantization to save space (~4x smaller, <1% recall loss)
  hnsw_m: 32
  hnsw_ef_construction: 40
  hnsw_ef_search: 32
  ivf_nlist: 4096
  ivf_nprobe: 32
  topk: 50
