experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: mteb/hotpotqa # Use 'mteb/hotpotqa' for MTEB format or 'hotpot_qa' for legacy
  graph_dir: ./datasets/hotpot
  graph_name: page_graph
  chunk_size: 220
  chunk_overlap: 40
  semantic_k: 15 # kNN augmentation: add k nearest neighbors in embedding space (set to null to disable)
  knn_only: true # If true, use only semantic edges (ablation)

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0
  train_batch_size: 256
  dev_batch_size: 1024
  eval_batch_size: 2048

graph:
  fanouts: [5, 2] # 2-hop neighbor sampling: sample up to 5 neighbors at hop-1, 2 at hop-2
  neg_seed_ratio: 1.0 # Number of negative seeds = neg_seed_ratio * num_positive_seeds
  hardneg_enabled: true # Hard negative mining from subgraph
  hardneg_ratio: 0.3 # Mine top 30% of available negatives (per query) as hard negatives
  hardneg_min: 32 # Minimum hard negatives per query (fallback for small subgraphs)
  hardneg_max: 128 # Maximum hard negatives per query (cap for very large subgraphs)

loss:
  tau: 0.1 # InfoNCE temperature
  tau_graph: 0.2 # Graph temperature
  lambda_q2d: 0.95 # Primary retrieval objective (graph structure via neighbor loss)
  alpha_link: 0.0

train:
  query_batch_size: 4
  gradient_accumulation_steps: 16
  epochs: 3
  lr_encoder: 1.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 20
  dev_every_steps: 100

dev:
  num_samples: 1000
  dev_corpus_size: 100000 # Small corpus for fast dev eval during training
  recall_k: 5
  seed: 42
  confuser_fraction: 0.1

eval:
  corpus_size: 500000 # null = full corpus, integer = sample for fast iteration (e.g., 500000)
  corpus_sample_seed: 42 # Fixed seed for reproducible sampling across methods
  recall_k: [5, 10]
  joint_recall_k: 10
  ndcg_k: 10
  mrr_k: 10

index:
  use_gpu: true
  use_fp16: false
  gpu_search_batch_size: 12
  topk: 50
