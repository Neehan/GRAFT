experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: hotpot_qa
  graph_path: ./datasets/hotpot/page_graph.pt
  chunk_size: 220
  chunk_overlap: 40

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0
  batch_size: 128

gnn:
  type: graphsage
  layers: 1
  hidden_dim: 768
  dropout: 0.1
  fanouts: [10]

loss:
  tau: 0.07
  tau_graph: 0.2
  lambda_q2d: 0.7
  alpha_link: 0.0

train:
  batch_size_queries: 32
  epochs: 2
  lr_encoder: 2.0e-5
  lr_gnn: 1.0e-3
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 50
  eval_every_steps: 500

eval:
  num_samples: 1000
  num_negatives: 300
  recall_k: 10

index:
  faiss_type: IVF,HNSW
  nlist: 4096
  nprobe: 32
  topk: 50

rag:
  generator: gpt-4o-mini
  topk_ctx: 10
  prompt_template: ./templates/hotpot.txt
