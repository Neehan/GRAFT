experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: mteb/hotpotqa # Use 'mteb/hotpotqa' for MTEB format or 'hotpot_qa' for legacy
  graph_dir: ./datasets/hotpot
  graph_name: page_graph
  chunk_size: 220
  chunk_overlap: 40
  batch_size: 2048 # Total batch size for inference/embedding (will be split across available GPUs with DataParallel)
  eval_batch_size: 2048 # Total batch size for evaluation queries (DataParallel splits encoding across GPUs)
  semantic_k: 3 # kNN augmentation: add k nearest neighbors in embedding space (set to null to disable)
  knn_only: true # If true, use only semantic edges (ablation)
  knn_index_type: hnsw # Index type for kNN search: "flat" (exact, slow) or "hnsw" (approximate, fast)

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0

graph:
  fanouts: [5, 2] # 2-hop neighbor sampling: sample up to 5 neighbors at hop-1, 2 at hop-2

loss:
  tau: 0.1 # InfoNCE temperature
  tau_graph: 0.2 # Graph temperature
  lambda_q2d: 0.9 # Primary retrieval objective (graph structure via neighbor loss)
  alpha_link: 0.0

train:
  query_batch_size: 4
  gradient_accumulation_steps: 16
  epochs: 3
  lr_encoder: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 20
  eval_every_steps: 100
  # Hard negative mining (subgraph-level)
  hardneg_enabled: true
  hardneg_per_query: 32

eval:
  num_samples: 500
  dev_corpus_size: 50000 # Small corpus for fast dev eval during training
  recall_k: 10 # Check recall@10 for more granular signal
  query_batch_size: 16
  encoder_batch_size: 128

index:
  type: hnsw # Index type: "flat", "hnsw", or "ivf"
  quantize: true # Use SQ8 quantization to save space (~4x smaller, <1% recall loss)
  hnsw_m: 32
  hnsw_ef_construction: 40
  hnsw_ef_search: 32
  ivf_nlist: 4096
  ivf_nprobe: 32
  ivf_train_sample_size: 1048576 # Sample size for IVF training (null = use all embeddings)
  sq8_train_sample_size: null # Sample size for SQ8 training (null = use all embeddings)
  add_batch_size: 50000 # Batch size for adding embeddings to index
  omp_threads: 32 # OpenMP threads for FAISS (tune for 256-core system)
  topk: 50
