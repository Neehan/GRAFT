experiment:
  name: graft_hotpot_e5_sage
  output_dir: ./outputs/graft_hotpot_e5_sage

data:
  dataset: mteb/hotpotqa # Use 'mteb/hotpotqa' for MTEB format or 'hotpot_qa' for legacy
  graph_dir: ./datasets/hotpot
  graph_name: page_graph
  chunk_size: 220
  chunk_overlap: 40
  semantic_k: 3 # kNN augmentation: add k nearest neighbors in embedding space (set to null to disable)
  knn_only: true # If true, use only semantic edges (ablation)

encoder:
  model_name: intfloat/e5-base-v2
  max_len: 256
  pool: cls
  freeze_layers: 0
  train_batch_size: 128
  dev_batch_size: 32
  eval_batch_size: 2048

graph:
  fanouts: [5, 2] # 2-hop neighbor sampling: sample up to 5 neighbors at hop-1, 2 at hop-2

loss:
  tau: 0.1 # InfoNCE temperature
  tau_graph: 0.2 # Graph temperature
  lambda_q2d: 0.9 # Primary retrieval objective (graph structure via neighbor loss)
  alpha_link: 0.0

train:
  query_batch_size: 4
  gradient_accumulation_steps: 16
  epochs: 3
  lr_encoder: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  grad_clip: 1.0
  bf16: true
  seed: 42
  log_every: 20
  eval_every_steps: 100
  # Hard negative mining (subgraph-level)
  hardneg_enabled: true
  hardneg_per_query: 32

eval:
  num_samples: 500
  dev_corpus_size: 100000 # Small corpus for fast dev eval during training
  recall_k: 5 # Check recall@10 for more granular signal
  query_batch_size: 8
  seed: 42
  confuser_fraction: 0.1

index:
  use_gpu: true
  use_fp16: false
  gpu_search_batch_size: 12
  topk: 50
